== Requirement ==
  * ruby
  * perl
  * blat
  * cross_match
  * large RAM

== Usage == 

The procedure has three steps:

1. Mapping and aligning 454 reads onto the reference genome.

2. Calling raw (a) SNPs and (b) indels based on results from step 1.

3. SNP/indel filtering based on quality scores and biology.


I have written a set of streamlined programs to do step 1 and step 2a. The step 2b (indels) is in the working. Step 3 is straightforward in terms of programming but requires some discussions on the biology.


*1. Atlas-mapper*

Map the reads (454 or Sanger) to reference sequence via BLAT and do refined sequence local alignment via cross_match.

1) Format the reference sequence (including making blat ooc file, splitting reference into smaller pieces for cross_match):

     `ruby /PATH/TO/atlas-mapper-format-ref.rb -r reference.fasta`

Note: this program requires about 3.0G RAM for the human genome.
The reference.fasta is the reference sequences in one fasta file. The output is a directory named by reference.fasta with a suffix "Env4mapping".

2) Map and align: (including doing blat, picking best hits and uniquely mapped reads, doing cross_match etc)
 
     `ruby /PATH/TO/atlas-mapper-do.rb -q query.fasta -r reference.fasta  [ options ]`

query.fasta is a fasta file containing a batch of 454 reads.


The output is a directory with prefix "Mapping_of_". It contains the blat result, blat best hits annotated as unique or repeats, and cross_match -discrep_lists output.

*Note* the query.fasta should be in reasonable size.  It's best to be no larger than 10Mbp. A typical 454 run produce 100+Mbp sequence. So it is necessary to split the run into smaller batches and do each batch on one cluster node.

Here is an Bash shell command example to submit all batches of mapping jobs to a cluster:

     `for f in batch_*.fa; do bsub -o lsf.o -e lsf.e -J $f ruby ..../atlas-mapper-do.rb -q $f -r ref.fa [ options ]; done `


*2. Atlas-SNP*

This program calls SNPs from the cross_match -discrep_list output produced from step 1:

     `ruby /PATH/TO/atlas-snp.rb -x cross_match_output -r reference.fasta -o prefix_of_outputs  [-a adjust_quality_slope]`

( The -x specifies the cross_match result concatenated from the batch results from step 1.  )

It outputs two files: one is a list of SNPs; the other is the coverage of each base of reference, formated similar to phred quality files.

The format of SNP list is like this:

`refName    coordinate  refBase homopolymer  refEnv  coverage SNPBase adjustedQual oriQual numVariant numAlter reads_info..`

`gi|49175990|    9776    T       5       ACACGGTATTTTT   1    A       9.41    15      1    1    A(15)213878_0447_0217(57)(222.0/228)-aaaaaaTccgtgt`

This file contains all the information required to filter SNPs. We can use a straightforward ruby/perl/shell script to do filtering and calling homozygous/heterozygous based on:

 # number of variant reads vs coverage
 # adjusted quality
 # the size of longest homopolymer run nearby

For more help information and other options, just type -h as the argument.

*2.1 Compute depth-coverage on reference *

    `ruby atlas-mapper-coverage.rb -x cross_match.output [-r ref.fasta] [ -t targeted_genomic_regions ] [ options ]`

 -r or -t is required. 
 -r specifies the reference fasta file. If provided, the program will compute the depth-coverage for each base on the reference. *NOTE* this is time-consuming for large genomes. 
 -t specifies a list of targeted genomic regions, with format like this:
  `chromosome    start_position    end_position`
      If provided, the program will compute the depth-coverage only for bases in the targeted regions. 